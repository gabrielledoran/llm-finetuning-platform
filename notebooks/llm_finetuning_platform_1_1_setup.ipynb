{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üê¨ ‚ú¶Àö‚Çä‚Äß‚Å∫Àñ <b>llm-finetuning-platform</b> Àñ‚Å∫‚Äß‚ÇäÀö‚ú¶\n"
      ],
      "metadata": {
        "id": "gMVy9gaCgOin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b>Part 1: Model setup</b>\n",
        "This notebook is intended to run on Google Colab A100 GPU."
      ],
      "metadata": {
        "id": "yt1amB7GsOxa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ks0cAK5UQDsp"
      },
      "outputs": [],
      "source": [
        "# packages\n",
        "!pip install -q transformers accelerate peft bitsandbytes datasets tensorboard scipy anthropic openai\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import bitsandbytes\n",
        "import datasets\n",
        "import datetime\n",
        "from datasets import load_dataset\n",
        "import huggingface_hub\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "import peft\n",
        "\n",
        "import anthropic\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get secrets\n",
        "from google.colab import userdata\n",
        "ANTHROPIC_API_KEY = userdata.get('anthropicsecretkey')\n",
        "HF_TOKEN = userdata.get('bab')\n",
        "OPENAI_API_KEY = userdata.get('openaisecretkey')\n",
        "\n",
        "# login to huggingface\n",
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "id": "NYAQlkMYQN4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "314c3e3d-784a-4b69-f402-7efa7f1394e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load in Llama"
      ],
      "metadata": {
        "id": "-armVzh0sP9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### from claude helper code\n",
        "# load model in\n",
        "model_name = \"meta-llama/Llama-3.1-8B\" ## originally:  \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "print(f\"Loading {model_name}... This may take a minute...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    dtype=torch.float16,  # fp16 saves memory vs fp32\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "# load in tokens\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "f5999a7ab54f432eb0bb68f709b62eac",
            "6f8f6fe2978146b4a3918e7c26a3b90c",
            "38ce3369e09e4f7fb7e36c6a57c49369",
            "f8bda740b4964343a9e4e205f97d70b8",
            "b8918f1f97d04b03897da533df8adc13",
            "81f15ff425c94e328167d00ecb2be886",
            "9d7ca58eda0742ca96367cabe035adb3",
            "349fa1a65e814c8a93e1270c923a43fd",
            "ec290be6ce834e7eb40f2c82bf134381",
            "10f10db16493433e991efe32030ce092",
            "4230e940d6b5407ca309bd9e15b99eff"
          ]
        },
        "id": "LmyOWFHHf4uB",
        "outputId": "9f976f81-1fa3-4a0a-c206-fe4272e6e6c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading meta-llama/Llama-3.1-8B... This may take a minute...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5999a7ab54f432eb0bb68f709b62eac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the ModelInterface method to select APIs"
      ],
      "metadata": {
        "id": "uyEvLT_ZshtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelInterface:\n",
        "    \"\"\"Unified interface for calling different models from APIs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.openai_client = openai.OpenAI(api_key=OPENAI_API_KEY) if 'OPENAI_API_KEY' in globals() else None\n",
        "        self.anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY) if 'ANTHROPIC_API_KEY' in globals() else None\n",
        "        self.local_model = model if 'model' in globals() else None\n",
        "        self.local_tokenizer = tokenizer if 'tokenizer' in globals() else None\n",
        "\n",
        "    def generate(self, prompt, model_name, max_tokens=150, temperature=0.7):\n",
        "        \"\"\"Generate response from specified model\"\"\"\n",
        "\n",
        "        if model_name == \"gpt-5.2\":\n",
        "            return self._generate_openai(prompt, \"gpt-5.2\", max_tokens, temperature)\n",
        "\n",
        "        elif model_name == \"gpt-3.5-turbo\":\n",
        "            return self._generate_openai(prompt, \"gpt-3.5-turbo\", max_tokens, temperature)\n",
        "\n",
        "        elif model_name == \"claude-haiku-4-5-20251001\":\n",
        "            return self._generate_anthropic(prompt, \"claude-haiku-4-5-20251001\", max_tokens, temperature)\n",
        "\n",
        "        elif model_name == \"claude-opus-4-5-20251101\":\n",
        "            return self._generate_anthropic(prompt, \"claude-opus-4-5\", max_tokens, temperature)\n",
        "\n",
        "        elif model_name == \"Llama-3.1-8B\":\n",
        "            return self._generate_local(prompt, max_tokens, temperature)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "    def _generate_openai(self, prompt, model, max_tokens, temperature):\n",
        "      \"\"\"Generate from OpenAI API. Select correct token argument per model.\"\"\"\n",
        "      kwargs = {\n",
        "          \"model\": model,\n",
        "          \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "          \"temperature\": temperature,\n",
        "          }\n",
        "      # newer models (\"gpt-5\") use max_completion_tokens instead of max_tokens\n",
        "      if model.startswith(\"gpt-5\"):\n",
        "          kwargs[\"max_completion_tokens\"] = max_tokens\n",
        "      else:\n",
        "          kwargs[\"max_tokens\"] = max_tokens\n",
        "      response = self.openai_client.chat.completions.create(**kwargs)\n",
        "      return response.choices[0].message.content\n",
        "\n",
        "    def _generate_anthropic(self, prompt, model, max_tokens, temperature):\n",
        "        \"\"\"Generate from Anthropic API.\"\"\"\n",
        "        response = self.anthropic_client.messages.create(\n",
        "            model=model,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response.content[0].text\n",
        "\n",
        "    def _generate_local(self, prompt, max_tokens, temperature):\n",
        "        \"\"\"Generate from local Llama model.\"\"\"\n",
        "        inputs = self.local_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.local_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=self.local_tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        response = self.local_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # remove the prompt from the response\n",
        "        response = response[len(prompt):].strip()\n",
        "        return response"
      ],
      "metadata": {
        "id": "Z5jhYACLhG3D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### last steps before all systems go! #####\n",
        "# initialize interface\n",
        "interface = ModelInterface()\n",
        "\n",
        "# test prompt to verify models working\n",
        "test_prompt = \"Complete this sentence: The secret chest finally opened, and \"\n",
        "models_list = [\"gpt-5.2\"]\n",
        "print(f\"--- testing large language models ---\\n\")\n",
        "print(f\"prompt: {test_prompt} ---\")\n",
        "for model_name in models_list:\n",
        "    try:\n",
        "        print(f\"\\n--- {model_name} ---\")\n",
        "        response = interface.generate(test_prompt, model_name)\n",
        "        print(f\"Response: {response[:100]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "xX8JF-cWHA-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7100cb8f-31e4-4d21-8b5d-55d75a77219b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- testing large language models ---\n",
            "\n",
            "prompt: Complete this sentence: The secret chest finally opened, and  ---\n",
            "\n",
            "--- gpt-5.2 ---\n",
            "Response: The secret chest finally opened, and a warm gust of air spilled out, carrying the scent of cedar and...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚òÅ\n",
        "\n",
        "\n",
        "#### ü©∞  ùìÖùìáùëíùìâùìâùìé ùëîùíæùìáùìÅùìà"
      ],
      "metadata": {
        "id": "JlUOCw8k_7LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt = \"Complete this sentence: I went into the attic of my grandparents house, and to my suprise I saw \"\n",
        "\n",
        "models_list = [\"gpt-5.2\", \"gpt-3.5-turbo\", \"Llama-3.1-8B\", \"claude-haiku-4-5-20251001\", \"claude-opus-4-5-20251101\"]\n",
        "print(f\"--- testing large language models ---\\n\")\n",
        "print(f\"prompt: {test_prompt} ---\")\n",
        "\n",
        "for model_name in models_list:\n",
        "    try:\n",
        "        print(f\"\\n--- {model_name} ---\")\n",
        "        response = interface.generate(test_prompt, model_name)\n",
        "        print(f\"response: {response[:1000]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"error: {e}\")\n",
        "\n",
        "print(\"\\n \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubs7OHE8-UPt",
        "outputId": "23504679-38f2-4749-bf79-552bcc7e5bed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- testing large language models ---\n",
            "\n",
            "prompt: Complete this sentence: I went into the attic of my grandparents house, and to my suprise I saw  ---\n",
            "\n",
            "--- gpt-5.2 ---\n",
            "response: I went into the attic of my grandparents' house, and to my surprise I saw **a dusty trunk with my name written on it in faded ink.**...\n",
            "\n",
            "--- gpt-3.5-turbo ---\n",
            "response:  a dusty old trunk filled with family heirlooms and photos from generations past....\n",
            "\n",
            "--- Llama-3.1-8B ---\n",
            "response: 3 ___________.\n",
            "A. ghosts\n",
            "B. skeletons\n",
            "C. cats\n",
            "D. toys\n",
            "E. none of the above\n",
            "Answer: B...\n",
            "\n",
            "--- claude-haiku-4-5-20251001 ---\n",
            "response: # I went into the attic of my grandparents house, and to my surprise I saw\n",
            "\n",
            "...a trunk filled with old photographs and letters dating back decades, along with my grandmother's wedding dress carefully preserved in tissue paper.\n",
            "\n",
            "---\n",
            "\n",
            "Would you like me to suggest other possible endings, or would you like to continue this story?...\n",
            "\n",
            "--- claude-opus-4-5-20251101 ---\n",
            "response: I went into the attic of my grandparents' house, and to my surprise I saw **a dusty old trunk I'd never noticed before, filled with faded photographs, handwritten letters tied with ribbon, and my grandmother's wedding dress carefully wrapped in yellowed tissue paper.**\n",
            "\n",
            "Would you like me to continue the story or offer some alternative endings?...\n",
            "\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scratch/Testing Area"
      ],
      "metadata": {
        "id": "1A7xZcdW8rR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############# scratch/testing #############\n",
        "##### trying to get notebook to display in github. trying to delete metadata. having problems with the file path for this notebook...? üòí\n",
        "import nbformat\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files from your Drive\n",
        "#drive.mount('/content/drive')\n",
        "notebook_to_process = \"/content/drive/llm_finetuning_platform_1_1_setup.ipynb\"\n",
        "output_notebook_name = \"llm_finetuning_platform_1_1_setup.ipynb\"\n",
        "\n",
        "if os.path.exists(notebook_to_process):\n",
        "    with open(notebook_to_process, \"r\", encoding=\"utf-8\") as f:\n",
        "        nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "    nb.metadata.pop(\"widgets\", None)\n",
        "\n",
        "    with open(output_notebook_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        nbformat.write(nb, f)\n",
        "\n",
        "    print(f\"Widget metadata removed from {notebook_to_process} and saved to {output_notebook_name}.\")\n",
        "else:\n",
        "    print(f\"Error: The file '{notebook_to_process}' was not found. Please ensure the path is correct and Google Drive is mounted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDPOZR5lqM6G",
        "outputId": "f212df01-4636-4eef-eac1-b2cd5de6810c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file '/content/drive/llm_finetuning_platform_1_1_setup.ipynb' was not found. Please ensure the path is correct and Google Drive is mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############# scratch/testing #############\n",
        "from openai import OpenAI\n",
        "from anthropic import Anthropic\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "client1 = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "\n",
        "#client.models.retrieve(\"gpt-5.2\")\n",
        "\n",
        "client1.models.list()\n",
        "#     id='claude-opus-4-5-20251101'\n",
        "#     display_name='Claude Opus 4.5'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4xc_PkS6Jwv",
        "outputId": "5616dad6-5df1-4487-df10-c00c5e70aa9d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncPage[ModelInfo](data=[ModelInfo(id='claude-opus-4-5-20251101', created_at=datetime.datetime(2025, 11, 24, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Opus 4.5', type='model'), ModelInfo(id='claude-haiku-4-5-20251001', created_at=datetime.datetime(2025, 10, 15, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Haiku 4.5', type='model'), ModelInfo(id='claude-sonnet-4-5-20250929', created_at=datetime.datetime(2025, 9, 29, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Sonnet 4.5', type='model'), ModelInfo(id='claude-opus-4-1-20250805', created_at=datetime.datetime(2025, 8, 5, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Opus 4.1', type='model'), ModelInfo(id='claude-opus-4-20250514', created_at=datetime.datetime(2025, 5, 22, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Opus 4', type='model'), ModelInfo(id='claude-sonnet-4-20250514', created_at=datetime.datetime(2025, 5, 22, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Sonnet 4', type='model'), ModelInfo(id='claude-3-haiku-20240307', created_at=datetime.datetime(2024, 3, 7, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Haiku 3', type='model')], has_more=False, first_id='claude-opus-4-5-20251101', last_id='claude-3-haiku-20240307')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save our work!"
      ],
      "metadata": {
        "id": "xHo_a7cA8jub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# document our setup/progress and save in model_setup.json\n",
        "from datetime import datetime\n",
        "\n",
        "setup_info = {\n",
        "    \"date\": datetime.now().isoformat(),\n",
        "    \"environment\": \"Google Colab\",\n",
        "    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
        "    \"models\": {\n",
        "        \"local\": [\"meta-llama/Llama-3.1-8B\"],\n",
        "        \"api\": [\"gpt-3.5-turbo\", \"gpt-5.2\", \"claude-haiku-4-5-20251001\", \"claude-opus-4-5-20251101\"] if 'OPENAI_API_KEY' in globals() else [\"None - add API keys\"],\n",
        "    },\n",
        "    \"memory_usage_gb\": round(torch.cuda.memory_allocated()/1e9, 2) if torch.cuda.is_available() else 0,\n",
        "}\n",
        "\n",
        "with open('model_setup.json', 'w') as f:\n",
        "    json.dump(setup_info, f, indent=2)"
      ],
      "metadata": {
        "id": "Izyf21YqlJMU"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}
